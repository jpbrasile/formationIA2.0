# Pour le fine tuning 
## Affiner un mod√®le avec ses donn√©es 
- [00:00](https://www.youtube.com/watch?v=jcABWwH1FBE&t=0s) ü§ñ Vous pouvez affiner le mod√®le Orca 2 pour r√©pondre aux questions de mani√®re personnalis√©e en utilisant des donn√©es d'entra√Ænement.
- [00:16](https://www.youtube.com/watch?v=jcABWwH1FBE&t=16s) üõ†Ô∏è Ce tutoriel d√©butant vous guide √©tape par √©tape pour affiner le mod√®le Orca 2.
- [00:30](https://www.youtube.com/watch?v=jcABWwH1FBE&t=30s) üì∫ Abonnez-vous √† la cha√Æne YouTube pour plus de vid√©os sur l'intelligence artificielle et n'oubliez pas de liker et de partager.
- [00:44](https://www.youtube.com/watch?v=jcABWwH1FBE&t=44s) üíª Installez les biblioth√®ques n√©cessaires via pip et cr√©ez un fichier app.py pour commencer.
- [01:10](https://www.youtube.com/watch?v=jcABWwH1FBE&t=70s) üìä Pr√©parez vos donn√©es d'entra√Ænement sous forme de questions et r√©ponses pour le mod√®le.
- [02:19](https://www.youtube.com/watch?v=jcABWwH1FBE&t=139s) üßÆ D√©terminez la longueur de s√©quence maximale en analysant vos donn√©es pour √©conomiser des ressources informatiques.
- [02:59](https://www.youtube.com/watch?v=jcABWwH1FBE&t=179s) ‚öôÔ∏è Configurez le mod√®le Ludwig en sp√©cifiant les caract√©ristiques d'entr√©e et de sortie, ainsi que d'autres param√®tres.
- [03:28](https://www.youtube.com/watch?v=jcABWwH1FBE&t=208s) üöÄ Entra√Ænez le mod√®le en utilisant les donn√©es d'entra√Ænement et sauvegardez les r√©sultats.
- [03:56](https://www.youtube.com/watch?v=jcABWwH1FBE&t=236s) üß™ Testez le mod√®le en pr√©disant des r√©ponses √† partir de donn√©es de test.
- [04:40](https://www.youtube.com/watch?v=jcABWwH1FBE&t=280s) üîÑ Le processus d'entra√Ænement du mod√®le est suivi avec l'affichage des scores de validation et de test.
- [05:36](https://www.youtube.com/watch?v=jcABWwH1FBE&t=336s) üåê Vous pouvez t√©l√©charger le mod√®le affin√© sur Hugging Face pour une utilisation ult√©rieure.

## Fine-tuning d'un mod√®le multimodal
- [00:03](https://youtu.be/usoTCfyQxjU?t=3s) üìö Le tutoriel se concentre sur le fine-tuning d'un mod√®le multimodal LLM appel√© "IDFICS 9B" pour la r√©ponse √† des questions visuelles.
- [01:21](https://youtu.be/usoTCfyQxjU?t=81s) üí° L'importance des LLM multimodaux, qui combinent le traitement du langage naturel et des donn√©es visuelles, est soulign√©e en raison des avanc√©es √† venir, telles que GPT-5 avec des capacit√©s multimodales am√©lior√©es.
- [03:39](https://youtu.be/usoTCfyQxjU?t=219s) üõ†Ô∏è Le processus de fine-tuning de "IDFICS 9B" est expliqu√©, avec l'utilisation de biblioth√®ques comme Bits and Bytes pour la quantification et des configurations sp√©cifiques. La vid√©o se concentre sur le fine-tuning de "IDFICS 9B" sur un ensemble de donn√©es Pok√©mon Go Cards.
- [24:28](https://www.youtube.com/watch?v=usoTCfyQxjU&t=24m28s) üöÄ Le processus de pr√©traitement des images dans la fine-tuning d'un mod√®le multimodal.
- [27:28](https://www.youtube.com/watch?v=usoTCfyQxjU&t=27m28s) üì∏ Comment cr√©er des prompts pour l'inf√©rence avec des images.
- [29:36](https://www.youtube.com/watch?v=usoTCfyQxjU&t=29m36s) üì¶ Comment charger et pr√©parer un jeu de donn√©es pour la fine-tuning d'un mod√®le multimodal.
- [40:17](https://www.youtube.com/watch?v=usoTCfyQxjU&t=40m17s) üí¨ Comment effectuer une inf√©rence avec un mod√®le fine-tun√© en utilisant une URL d'image et une question.
- [44:07](https://www.youtube.com/watch?v=usoTCfyQxjU&t=44m07s) üì§ Comment pousser un mod√®le fine-tun√© vers Hugging Face Hub pour le partager avec la communaut√©.

## Biblioth√®que open source pour l'entra√Ænement et la fine-tuning de mod√®les de vision
Le site [Deci-AI/super-gradients](https://github.com/Deci-AI/super-gradients?tab=readme-ov-file) propose une biblioth√®que open source pour l'entra√Ænement et la fine-tuning de mod√®les de vision par ordinateur de pointe, avec un accent sur le mod√®le YOLO-NAS.

- Le site Deci-AI/super-gradients offre une biblioth√®que open source pour l'entra√Ænement de mod√®les de vision par ordinateur de pointe.
- Le mod√®le YOLO-NAS est mis en avant comme une caract√©ristique principale de la biblioth√®que.
- La biblioth√®que prend en charge diverses t√¢ches de vision par ordinateur, telles que la classification, la segmentation s√©mantique, la d√©tection d'objets et l'estimation de pose.
- Il est possible de charger des mod√®les pr√©-entra√Æn√©s et de les affiner pour des performances optimales.
- La biblioth√®que est compatible avec des outils de d√©ploiement tels que TensorRT et OpenVINO.
- Des tutoriels et des recettes sont disponibles pour aider les utilisateurs √† d√©marrer rapidement.
- La biblioth√®que est constamment mise √† jour, avec la derni√®re version √©tant la 3.5.0.
- Le projet est sous licence Apache 2.0 et est h√©berg√© sur GitHub.
- Une plateforme appel√©e Deci Platform est √©galement mentionn√©e, offrant des fonctionnalit√©s pour la compilation et la quantification automatiques de mod√®les.

Le site propose une biblioth√®que compl√®te pour l'entra√Ænement et la fine-tuning de mod√®les de vision par ordinateur, avec un accent sur le mod√®le YOLO-NAS.
# Pour la "quantification
- [00:00](https://youtu.be/Kj0OIkWpfHs?t=0s) üìã Introduction √† la quantification de mod√®le

  - La quantification de mod√®le est le processus de mise en correspondance des valeurs continues avec des valeurs discr√®tes pour l'apprentissage automatique.
  - La quantification permet d'ex√©cuter de grands mod√®les de langage sur les CPU et de d√©charger certaines couches vers les GPU pour acc√©l√©rer les calculs.
  - Le format le plus populaire de quantification de nos jours est le TTF (Transformers Tensilica Format).

- [02:45](https://youtu.be/Kj0OIkWpfHs?t=165s) üéØ Pr√©paration de l'environnement et du mod√®le

  - Configuration d'un environnement Google Colab avec un GPU T4 gratuit et un compte Hugging Face.
  - D√©finition du mod√®le √† quantifier (Neural BigLE, 147 milliards de param√®tres).

- [06:47](https://youtu.be/Kj0OIkWpfHs?t=407s) üíæ Conversion en virgule flottante 16 bits

  - La conversion du mod√®le en virgule flottante 16 bits r√©duit la consommation de m√©moire et am√©liore l'efficacit√© √©nerg√©tique.

- [08:39](https://youtu.be/Kj0OIkWpfHs?t=519s) ‚öôÔ∏è Processus de quantification

  - Exploration de la m√©thode de quantification Q4 KM.
  - Quantification du mod√®le couche par couche, r√©duisant les besoins en m√©moire et le temps de traitement.

- [11:23](https://youtu.be/Kj0OIkWpfHs?t=683s) üì§ T√©l√©chargement du mod√®le quantifi√© sur Hugging Face

  - Connexion √† Hugging Face, initialisation de l'API et cr√©ation d'un r√©f√©rentiel pour le mod√®le quantifi√©.
  - T√©l√©chargement du mod√®le quantifi√© dans le r√©f√©rentiel Hugging Face.

- [12:52](https://youtu.be/Kj0OIkWpfHs?t=772s) üßê V√©rification du mod√®le t√©l√©charg√©

  - V√©rification du mod√®le quantifi√© t√©l√©charg√© dans le r√©f√©rentiel Hugging Face.
  - Confirmation du succ√®s du processus de quantification et de t√©l√©chargement.
 
  ## RoSA : mieux que LORA
  - [00:00](https://www.youtube.com/watch?v=p1ER6aNkEMQ&t=0s) üîç Introduction √† la m√©thode Rosa en PFT

  - La PFT (Parameter Efficient Tuning) est une technique de ML,
  - La m√©thode Rosa est une adaptation √† faible rang qui pr√©tend surpasser Laura,
  - Rosa est bas√©e sur l'analyse robuste des composantes principales.

- [01:20](https://www.youtube.com/watch?v=p1ER6aNkEMQ&t=80s) üìä Performance de Rosa dans des t√¢ches g√©n√©ratives

  - Rosa a surpass√© Laura et la fine-tuning sparse dans les t√¢ches g√©n√©ratives,
  - Ils ont introduit un support syst√®me pour Rosa, notamment des GPU √©pars pour une efficacit√© m√©moire et computationnelle,
  - Le code de Rosa n'est pas encore disponible, ce qui est un inconv√©nient.

- [03:08](https://www.youtube.com/watch?v=p1ER6aNkEMQ&t=188s) üßê R√©sum√© et attente du code

  - Pr√©sentation des comparaisons de m√©moire entre Lama, fft, Laura, Spa et Rosa,
  - Le code de Rosa n'est pas encore disponible, ce qui est une pr√©occupation,
  - L'auteur invite les commentaires et les abonnements √† la cha√Æne.
